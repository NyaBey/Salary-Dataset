# Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# 1. Load Dataset
file_path = r"C:\Users\yunia\OneDrive\Dokumente\DS PROJECTS PY\Salary_dataset.csv"

# Check if the file exists before proceeding
if not os.path.exists(file_path):
    print("Error: File not found. Check the file path.")
    exit()

# Load dataset safely
df = pd.read_csv(file_path)

# Remove unnecessary column if it exists
if "Unnamed: 0" in df.columns:
    df = df.drop(columns=["Unnamed: 0"])

# Display dataset structure
print("Dataset Loaded Successfully!")
print("Columns:", df.columns)
print("Missing Values:\n", df.isnull().sum())
print(df.head())

Unnamed: 0  YearsExperience   Salary
0           0              1.2  39344.0
1           1              1.4  46206.0
2           2              1.6  37732.0
3           3              2.1  43526.0
4           4              2.3  39892.0

The dataset contains the following columns:
•	Unnamed: 0: An index column that seems unnecessary.
•	YearsExperience: The number of years of experience.
•	Salary: The salary corresponding to the experience.

# Remove the unnecessary column
df = df.drop(columns=["Unnamed: 0"])

# Display summary statistics
df.describe()

Result
Years		Experience 		Salary 
count 		30.000000 		30.000000 
mean 		5.413333 		76004.000000 
std 		2.837888 		27414.429785 
min 		1.200000 		37732.000000 
25% 		3.300000 		56721.750000 
50% 		4.800000 		65238.000000 
75% 		7.800000 		100545.750000 
max 		10.600000 		122392.000000

Summary of the Salary Dataset:
•	Years of Experience:
o	Ranges from 1.2 years to 10.6 years.
o	Mean experience: 5.41 years.
o	Standard deviation: 2.84 years.

•	Salary:
o	Ranges from 37,732 to 122,392.
o	Mean salary: 76,004.
o	Standard deviation: 27,414.

# 2. Define Features & Target
X = df[["YearsExperience"]]
y = df["Salary"]

# Perform Train-Test Split (80% Train, 20% Test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training Set Shape:", X_train.shape)
print("Testing Set Shape:", X_test.shape)

# 3. Train & Evaluate Linear Regression Model
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

y_pred_linear = linear_model.predict(X_test)

# Linear Regression Metrics
mae_linear = mean_absolute_error(y_test, y_pred_linear)
mse_linear = mean_squared_error(y_test, y_pred_linear)
rmse_linear = np.sqrt(mse_linear)
r2_linear = r2_score(y_test, y_pred_linear)

print("\n Linear Regression Performance:")
print(f"MAE: {mae_linear:.2f}, MSE: {mse_linear:.2f}, RMSE: {rmse_linear:.2f}, R² Score: {r2_linear:.4f}")

Training Set Shape: (24, 1)
Testing Set Shape: (6, 1)
Model Coefficients: [9423.81532303]
Model Intercept: 24380.20147947369
Model Evaluation Metrics:
Mean Absolute Error (MAE): 6286.453830757745
Mean Squared Error (MSE): 49830096.855908334
Root Mean Squared Error (RMSE): 7059.043621901506
R² Score: 0.9024461774180498

# 4. Visualize Linear Regression
plt.figure(figsize=(8, 5))
sns.regplot(x=df["YearsExperience"], y=df["Salary"], ci=None, line_kws={"color": "red"})
plt.xlabel("Years of Experience")
plt.ylabel("Salary")
plt.title("Salary vs Years of Experience (Linear Regression)")
plt.grid(True)
plt.show()

# 5. Train & Evaluate Polynomial Regression (Degree 3)
degree = 3
poly_model = make_pipeline(PolynomialFeatures(degree), LinearRegression())
poly_model.fit(X_train, y_train)

y_pred_poly = poly_model.predict(X_test)

# Polynomial Regression Metrics
mae_poly = mean_absolute_error(y_test, y_pred_poly)
mse_poly = mean_squared_error(y_test, y_pred_poly)
rmse_poly = np.sqrt(mse_poly)
r2_poly = r2_score(y_test, y_pred_poly)

print("\n Polynomial Regression (Degree 3) Performance:")
print(f"MAE: {mae_poly:.2f}, MSE: {mse_poly:.2f}, RMSE: {rmse_poly:.2f}, R² Score: {r2_poly:.4f}")
Polynomial Regression (Degree 2) Performance:
Mean Absolute Error (MAE): 4653.06904474681
Mean Squared Error (MSE): 31257508.450947188
Root Mean Squared Error (RMSE): 5590.841479683286
R² Score: 0.956975168321065

# 6. Visualize Polynomial Regression
df["Predicted_Salary_Poly"] = poly_model.predict(df[["YearsExperience"]])

plt.figure(figsize=(8, 5))
sns.scatterplot(x=df["YearsExperience"], y=df["Salary"], label="Actual Salary", color="blue", alpha=0.7)
sns.lineplot(x=df["YearsExperience"], y=df["Predicted_Salary_Poly"], color="green", label="Polynomial Regression (Degree 3)")
plt.xlabel("Years of Experience")
plt.ylabel("Salary")
plt.title("Polynomial Regression (Degree 3)")
plt.legend()
plt.grid(True)
plt.show()

# 7. Train Ridge & Lasso Regression Models (Regularization)
ridge_model = make_pipeline(PolynomialFeatures(3), Ridge(alpha=1))
ridge_model.fit(X_train, y_train)
ridge_r2 = r2_score(y_test, ridge_model.predict(X_test))

lasso_model = make_pipeline(PolynomialFeatures(3), Lasso(alpha=100, max_iter=5000))
lasso_model.fit(X_train, y_train)
lasso_r2 = r2_score(y_test, lasso_model.predict(X_test))

# 8. Model Comparison
model_comparison = pd.DataFrame({
    "Model": ["Linear Regression", "Polynomial Regression (Degree 3)", 
              "Ridge Regression (α=1)", "Lasso Regression (α=100)"],
    "Test R² Score": [r2_linear, r2_poly, ridge_r2, lasso_r2]
})

# 9. Visualize Model Comparison
plt.figure(figsize=(8, 5))
sns.barplot(y="Model", x="Test R² Score", data=model_comparison, palette="coolwarm")
plt.xlabel("R² Score (Higher is Better)")
plt.ylabel("Model")
plt.title("Model Comparison: R² Scores")
plt.grid(True)
plt.show()

# 10. Feature Importance from Lasso Model
lasso_final_model = Lasso(alpha=100, max_iter=5000)
poly = PolynomialFeatures(3)
X_train_poly = poly.fit_transform(X_train)
lasso_final_model.fit(X_train_poly, y_train)

# Get polynomial feature names
feature_names = poly.get_feature_names_out(["YearsExperience"])
lasso_coefficients = lasso_final_model.coef_

# Create a DataFrame for feature importance
feature_importance_df = pd.DataFrame({"Feature": feature_names, "Coefficient": lasso_coefficients})
feature_importance_df = feature_importance_df.sort_values(by="Coefficient", ascending=False)

# 11. Visualize Feature Importance
plt.figure(figsize=(8, 5))
sns.barplot(y="Feature", x="Coefficient", data=feature_importance_df, palette="coolwarm")
plt.xlabel("Coefficient Value")
plt.ylabel("Feature")
plt.title("Feature Importance (Lasso Regression)")
plt.grid(True)
plt.show()

# 12. Save Results
model_comparison.to_csv("model_comparison_results.csv", index=False)
feature_importance_df.to_csv("lasso_feature_importance.csv", index=False)

print("\n Model comparison saved to 'model_comparison_results.csv'")
print("\n Lasso feature importance saved to 'lasso_feature_importance.csv'")

# 13. Print Final Results
print("\n Final Model Comparison:")
print(model_comparison)

print("\n Lasso Feature Importance:")
print(feature_importance_df)
